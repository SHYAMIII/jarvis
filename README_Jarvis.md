
# Jarvis â€“ Your Python LLM Voice Assistant ğŸ”ŠğŸ¤–

An interactive voice assistant built in Python, powered by a Mistral-style LLM and Google Textâ€‘toâ€‘Speech (gTTS) for live conversational responses.

## ğŸš€ Features

- **Large Language Model (LLM) integration**  
  Sends spoken user input to a Mistral-compatible LLM API and gets text responses.

- **Text-to-Speech (TTS) via gTTS**  
  Converts LLM-generated responses into speech that plays back to the user.

- **Live audio interaction**  
  Accepts microphone input, transcribes (if you added STT), queries LLM, and speaks answerâ€”simulating a â€œJarvisâ€ experience.

## ğŸ§© Project Structure

```
jarvis/
â”œâ”€â”€ main.py              # Core flow: capture input â†’ query LLM â†’ speak response
â”œâ”€â”€ mistral_client.py    # Handles LLM API integration (model, key, endpoint)
â”œâ”€â”€ tts_handler.py       # Uses gTTS, saves playback-ready audio
â”œâ”€â”€ requirements.txt     # Dependencies (httpx, gtts, etc.)
â”œâ”€â”€ .env                 # Stores MISTRAL_API_KEY, optional configs
â””â”€â”€ README.md            # This file
```

## âš™ï¸ Installation & Setup

1. Clone the repo:
   ```bash
   git clone https://github.com/SHYAMIII/jarvis.git
   cd jarvis
   ```

2. Create and activate a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate       # Linux/macOS
   venv\Scripts\activate.bat      # Windows
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Add your `.env` file with:
   ```
   MISTRAL_API_KEY=your_api_key_here
   ```

## ğŸ How to Run

Start the assistant:

```bash
python main.py
```

**Workflow**:

1. Speak into the mic to ask a question.  
2. Audio is optionally transcribed or directly sent to the LLM.  
3. The LLM returns a text reply.  
4. gTTS converts it to speech and plays it back.

## ğŸ¤– How It Works

- **`main.py`** orchestrates the flow: listen â†’ query â†’ speak.
- **`mistral_client.py`** sends your prompt to the Mistral API and fetches completion.
- **`tts_handler.py`** uses gTTS to synthesize voice from text, saved as audio or played live.

## ğŸ“ Example Usage

```python
# main.py (simplified)
from mistral_client import get_llm_response
from tts_handler import speak_text

while True:
    prompt = input("You: ")
    if prompt.lower() in ("exit", "quit"):
        break
    reply = get_llm_response(prompt)
    print("Jarvis:", reply)
    speak_text(reply)
```

## ğŸ“¦ Dependencies

- `httpx` â€“ for async LLM API requests  
- `gtts` â€“ Google Textâ€‘toâ€‘Speech  
- `playsound` or similar â€“ for audio playback  
- (Optional) STT library if you included transcription  

See `requirements.txt` for exact versions.

## ğŸ”§ Customization & Extensions

- Swap out LLM â€“ support other models or endpoints  
- Enhance STT â€“ use Whisper/Vosk/STT  
- Add hotword activation (e.g., â€œHey Jarvisâ€¦â€)  
- Save conversation logs or adapt personalities

## ğŸ“ License

MIT License â€” see the `LICENSE` file.  
Youâ€™re welcome to use, modify, fork, and contribute improvements!
